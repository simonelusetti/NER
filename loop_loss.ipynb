{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_entity_matrix(text, ade_list):\n",
    "    \"\"\"\n",
    "    Create an entity matrix for a given sentence and ADE list.\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    n = len(words)\n",
    "    \n",
    "    # Initialize NxN matrix\n",
    "    entity_matrix = np.zeros((n, n), dtype=int)\n",
    "\n",
    "    # Mark entity spans\n",
    "    for ade in ade_list:\n",
    "        ade_tokens = ade.split()\n",
    "        current_token = 0\n",
    "        current_word = 0\n",
    "        starting_word = -1\n",
    "        # Locate entity indices\n",
    "        if len(ade_tokens) == 1:\n",
    "            for i in range(n):\n",
    "                if words[i] == ade_tokens[0]:\n",
    "                    entity_matrix[i][i] = 1\n",
    "        else:\n",
    "            while current_word < n and current_token < len(ade_tokens)-1:\n",
    "                if words[current_word] == ade_tokens[current_token]:\n",
    "                    if starting_word == -1:\n",
    "                        starting_word = current_word\n",
    "                    next_not_found = True\n",
    "                    next_word = current_word\n",
    "                    while next_word < n and next_not_found:\n",
    "                        if words[next_word] == ade_tokens[current_token+1]:\n",
    "                            entity_matrix[current_word][next_word] = 1\n",
    "                            current_token += 1\n",
    "                            if current_token == len(ade_tokens)-1:\n",
    "                                entity_matrix[next_word][starting_word] = 1\n",
    "                            next_not_found = False\n",
    "                        next_word += 1\n",
    "                current_word += 1\n",
    "                \n",
    "    return entity_matrix\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def extract_entities_from_matrix(text, entity_matrix):\n",
    "    \"\"\"\n",
    "    Extract entities from a binary matrix by constructing a tree of next-word connections\n",
    "    and using the lower triangle to detect and close entity loops.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input sentence.\n",
    "        entity_matrix (np.ndarray): The NxN entity matrix.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A list of extracted entities.\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    n = len(words)\n",
    "    entities = set()\n",
    "\n",
    "    # Step 1: Construct adjacency tree from upper triangle (forward connections)\n",
    "    entity_tree = {i: [] for i in range(n)}\n",
    "\n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            if entity_matrix[i, j] == 1:  # Upper triangle connection\n",
    "                entity_tree[i].append(j)\n",
    "\n",
    "    # Step 2: Identify entity loops from lower triangle (closing connections)\n",
    "    closing_loops = [(j, i) for i in range(n) for j in range(i + 1, n) if entity_matrix[j, i] == 1]\n",
    "\n",
    "    # Step 3: Traverse the tree and extract entities\n",
    "    for end_word, start_word in closing_loops:\n",
    "        stack = [(start_word, [start_word])]  # Stack to track traversal paths\n",
    "\n",
    "        while stack:\n",
    "            current_word, path = stack.pop()\n",
    "\n",
    "            # If we reach the closing loop, finalize the entity\n",
    "            if current_word == end_word:\n",
    "                entity = \" \".join(words[idx] for idx in path)\n",
    "                entities.add(entity)\n",
    "                continue  # Stop extending this path\n",
    "\n",
    "            # Traverse only valid next-word connections\n",
    "            for next_word in entity_tree[current_word]:\n",
    "                if next_word not in path and next_word <= end_word:  # Ensure it stays within entity boundaries\n",
    "                    stack.append((next_word, path + [next_word]))\n",
    "\n",
    "    for i in range(n):\n",
    "        if entity_matrix[i, i] == 1:\n",
    "            entities.add(words[i])\n",
    "\n",
    "    return list(entities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Sample ---\n",
      "Text: i have vaginal bleeding and my vaginal skin burns a lot my cycle was back im menopausal  breast pain other symptoms i have erithema my skin is so dry i have little cuts on my hands i m sweating a lot usually i dont pruritus ani etc i took it 3 times the first for 3 days then i stopped because it didnt make great improvements and i had diarrhea my doctor told me the symptoms had to go away after taking few more second time i took other 3 pills in 3 days little improvement with the pain i had the first vaginal bleeding a lot of pruritus and my skin was all red but i didnt know it was arthrotec my doctor prescribed me a topical cream  did some vaginal test to exclude infections i had my cycle the last one for 4 days more problems and this time i had the idea to go on internet and i discovered i had a lot of the side effects by taking arthrotec\n",
      "Actual ADEs: ['skin is so dry', 'vaginal skin burns', 'breast pain', 'pruritus', 'skin was all red', 'vaginal bleeding', 'sweating a lot', 'erithema', 'diarrhea']\n",
      "Extracted Entities: ['vaginal bleeding', 'erithema', 'skin was all red', 'diarrhea', 'breast pain', 'skin is so dry', 'pruritus', 'sweating a lot', 'vaginal skin burns']\n",
      "Match: True\n",
      "----------------------\n",
      "\n",
      "--- Sample ---\n",
      "Text: have been on lipitor over 5 yrs as time has passed i have developed foot shin knee thigh and hip pain shoulder arm and hand pain  am having balance problems clumsiness and some memory loss i have had problem with bloating and flatulence big time dr just took me off lipitor and effexor for depression only 2 weeks off so no noticeable change as yet had an mri last week to rule out multiple sclerosis or any brain problems i hope  have not gotten results as yet lipitor reduced my cholesterol total greatly from 274 to 160 hdl went up and ldl and triglicerides went down but looks like the side effects are going to cause me to change meds or quit taking them do the gym 3x a week and walk am not totally sedentary and watch what i eat but i am a cholesterol mfg so is my sister right now i am just hurting and want to find out why stay tuned\n",
      "Actual ADEs: ['clumsiness', 'flatulence', 'shoulder pain', 'foot pain', 'bloating', 'thigh pain', 'hand pain', 'hip pain', 'shin pain', 'balance problems', 'arm pain', 'knee pain', 'memory loss']\n",
      "Extracted Entities: ['knee pain', 'shin pain', 'shoulder pain', 'bloating', 'thigh pain', 'balance problems', 'clumsiness', 'flatulence', 'foot pain', 'memory loss', 'arm pain', 'hip pain', 'hand pain']\n",
      "Match: True\n",
      "----------------------\n",
      "\n",
      "--- Sample ---\n",
      "Text: tingling in feet  faceabdominal  muscular pain in back of neck cramps in calves this stuff is horrible  you have to fight with your doctor not to take this stuff get the word out\n",
      "Actual ADEs: ['tingling', 'cramps in calves', 'muscular pain']\n",
      "Extracted Entities: ['tingling', 'cramps in calves', 'muscular pain']\n",
      "Match: True\n",
      "----------------------\n",
      "\n",
      "--- Sample ---\n",
      "Text: i have been taking simvostatin for about 5 yearssince march 2010 i have been experiencing pain in my left leg and foot with exteem pain in my ankle and at times i have had dificultly in walking to the point were on some days i could not walk at all because the pain was so bad also had pain in my left arm as if i had strained it lifting but i havnt been lifting this is not like me because i am normaly very fit and i love walking doctor refered me to hostpital for xray on my foot which proved normal after reading some of the threads on here i decided not to take my nightly statin 20mgr since i decided to give them a miss i find that the pain in my foot is subsiding and i can walk a lot better i never had this problen with my foot and arm untill i took this drugi mentioned this to my doctor who said in not many words that i should have no side effects from simvostatin well i have proved him wrong i can only hope that having taken this drug for 5 years that not to much d\n",
      "Actual ADEs: ['could not walk', 'pain in my left leg and foot', 'pain in my foot', 'dificultly in walking', 'pain in my left leg and foot', 'exteem pain in my ankle', 'pain', 'exteem pain in my ankle']\n",
      "Extracted Entities: ['could not walk', 'exteem pain in my ankle', 'pain in my foot', 'pain in my left leg and foot', 'pain', 'dificultly in walking']\n",
      "Match: True\n",
      "----------------------\n",
      "\n",
      "--- Sample ---\n",
      "Text: feels like menstrual cramps back ache sick to stomach for 24 hours  was unhappy about being prescribed this but gave it a try since the pharmacy couldnt take it back mistake  didnt read about it until after id picked up the prescription after taking pill with food feels like menstrual cramps lower back pain  nausea within 45 minutes after the last pill have been sick to my stomach for over 24 hours discontinuing use immediately have heard great things about celebrex maybe will ask for that\n",
      "Actual ADEs: ['menstrual cramps', 'sick to stomach', 'back ache', 'sick to my stomach', 'nausea', 'lower back pain']\n",
      "Extracted Entities: ['back ache', 'sick to my stomach', 'menstrual cramps', 'nausea', 'sick to stomach', 'lower back pain']\n",
      "Match: True\n",
      "----------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12b9f5a8b5004044a64edf56b7f110ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/879 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset saved successfully in Hugging Face format!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "\n",
    "# Load the CADEC dataset\n",
    "cadec = load_dataset(\"KevinSpaghetti/cadec\")\n",
    "\n",
    "# Convert dataset to DataFrame\n",
    "df = pd.DataFrame(cadec[\"train\"])\n",
    "\n",
    "# Function to remove punctuation\n",
    "def normalize_text(text):\n",
    "    return re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "# Apply normalization to both text and ADEs\n",
    "grouped_df = df.groupby(\"text\")[\"ade\"].apply(list).reset_index()\n",
    "grouped_df[\"text\"] = grouped_df[\"text\"].apply(normalize_text)\n",
    "grouped_df[\"ade\"] = grouped_df[\"ade\"].apply(lambda ade_list: [normalize_text(ade) for ade in ade_list])\n",
    "\n",
    "# Apply transformation to generate entity matrices\n",
    "grouped_df[\"entity_matrix\"] = grouped_df.apply(lambda row: create_entity_matrix(row[\"text\"], row[\"ade\"]), axis=1)\n",
    "\n",
    "# Convert each matrix to a 1D list format\n",
    "grouped_df[\"entity_matrix\"] = grouped_df[\"entity_matrix\"].apply(lambda matrix: matrix.flatten().tolist())\n",
    "\n",
    "# Sanity check on 5 random samples\n",
    "sample_df = grouped_df.sample(5, random_state=42)\n",
    "\n",
    "for index, row in sample_df.iterrows():\n",
    "    text = row[\"text\"]\n",
    "    flat_matrix = row[\"entity_matrix\"]\n",
    "\n",
    "    # Convert back to a 2D matrix\n",
    "    words = text.split()\n",
    "    size = len(words)\n",
    "    matrix = np.array(flat_matrix).reshape((size, size))\n",
    "    \n",
    "    extracted_entities = extract_entities_from_matrix(text, matrix)\n",
    "\n",
    "    print(\"\\n--- Sample ---\")\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Actual ADEs: {row['ade']}\")\n",
    "    print(f\"Extracted Entities: {extracted_entities}\")\n",
    "    print(f\"Match: {set(extracted_entities) == set(row['ade'])}\")\n",
    "    print(\"----------------------\")\n",
    "\n",
    "# Convert the dataset to Hugging Face format\n",
    "dataset = Dataset.from_pandas(grouped_df)\n",
    "\n",
    "# Save the dataset\n",
    "dataset_dict = DatasetDict({\"train\": dataset})\n",
    "dataset_dict.save_to_disk(\"./datasets/cadec\")\n",
    "\n",
    "print(\"\\nDataset saved successfully in Hugging Face format!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes that are part of a loop: [False False False False False]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def reachability(m,k):\n",
    "    return np.diag(np.linalg.matrix_power(m,k))\n",
    "\n",
    "def find_loops(adj_matrix):\n",
    "    n = adj_matrix.shape[0]\n",
    "\n",
    "    # Nodes that are NOT part of any cycle (diagonal elements are zero)\n",
    "    m = [reachability(adj_matrix, i) for i in range(1, n+1)]\n",
    "\n",
    "    return (np.ones((1,n)) @ m).flatten().tolist()\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def split_lower_triangular_ones(matrix):\n",
    "    # Extract the upper triangular part (including diagonal)\n",
    "    upper_triangle = np.triu(matrix)\n",
    "    \n",
    "    # Find positions of 1s in the lower triangle\n",
    "    lower_positions = np.argwhere(np.tril(matrix, k=-1) == 1)\n",
    "    \n",
    "    # Generate new matrices, each keeping only one lower-triangle 1\n",
    "    matrices = []\n",
    "    for (i, j) in lower_positions:\n",
    "        new_matrix = np.copy(upper_triangle)  # Copy upper triangle\n",
    "        new_matrix[i, j] = 1  # Place one 1 in lower triangle\n",
    "        matrices.append(new_matrix)\n",
    "    \n",
    "    return matrices\n",
    "\n",
    "def nodes_with_arcs(adj_matrix):\n",
    "    return np.any(adj_matrix, axis=0) | np.any(adj_matrix, axis=1)\n",
    "\n",
    "# Example Usage\n",
    "A = np.array([\n",
    "    [0, 1, 0, 0, 1],  \n",
    "    [0, 0, 1, 0, 0],  \n",
    "    [1, 0, 0, 0, 0],  \n",
    "    [0, 0, 0, 0, 0],\n",
    "    [1, 0, 0, 0, 0]   \n",
    "])\n",
    "\n",
    "x = np.any([find_loops(a) for a in split_lower_triangular_ones(A)],axis=0)\n",
    "x = ~(x ^ (~nodes_with_arcs(A)))\n",
    "\n",
    "print(f\"Nodes that are part of a loop: {x}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "\n",
    "def collate_fn(batch, tokenizer, max_seq_length=128):\n",
    "    \"\"\"\n",
    "    Collate function to pad words and entity matrices within a batch.\n",
    "\n",
    "    Args:\n",
    "        batch: List of tuples (words, entity_matrix).\n",
    "        tokenizer: Tokenizer to encode and pad words.\n",
    "        max_seq_length: Maximum sequence length for padding.\n",
    "\n",
    "    Returns:\n",
    "        - Encoded tokenized inputs (input_ids, attention_mask, word_ids).\n",
    "        - Padded entity matrices tensor.\n",
    "    \"\"\"\n",
    "    # Unpack tuples\n",
    "    words_batch, matrices_batch, entities = zip(*batch)\n",
    "\n",
    "    # Tokenize and pad words\n",
    "    encoded = tokenizer(\n",
    "        list(words_batch),\n",
    "        is_split_into_words=True,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=max_seq_length\n",
    "    )\n",
    "\n",
    "    # Determine batch size and padded sequence length\n",
    "    batch_size = len(batch)\n",
    "    padded_seq_len = max_seq_length\n",
    "\n",
    "    # Initialize a zero-padded entity matrix (batch_size, max_seq_length, max_seq_length)\n",
    "    padded_matrices = torch.zeros((batch_size, padded_seq_len, padded_seq_len), dtype=torch.float32)\n",
    "\n",
    "    for i, (matrix, words) in enumerate(zip(matrices_batch, words_batch)):\n",
    "        size = len(words)\n",
    "        matrix = matrix.reshape((size, size))\n",
    "        seq_len = min(matrix.shape[0], padded_seq_len)  # Ensure we don't exceed max size\n",
    "        padded_matrices[i, :seq_len, :seq_len] = matrix[:seq_len, :seq_len]\n",
    "        \n",
    "    return encoded, padded_matrices, entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import os\n",
    "import json\n",
    "\n",
    "class EntityMatrixDataset(Dataset):\n",
    "    def __init__(self, data_dir, tokenizer, max_seq_length=128):\n",
    "        \"\"\"\n",
    "        Initialize the dataset by loading all JSON files from the directory.\n",
    "        \n",
    "        Args:\n",
    "            data_dir: Path to the directory containing JSON files.\n",
    "            tokenizer: Tokenizer to encode tokens.\n",
    "            max_seq_length: Maximum sequence length for padding/truncation.\n",
    "        \"\"\"\n",
    "        self.data = []\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_seq_length = max_seq_length\n",
    "\n",
    "        for file_name in os.listdir(data_dir):\n",
    "            if file_name.endswith(\".json\"):\n",
    "                with open(os.path.join(data_dir, file_name), \"r\") as f:\n",
    "                    entry = json.load(f)\n",
    "                    \n",
    "                    # **Extract only the single-word entity labels**\n",
    "                    entry[\"entity_matrix\"] = entry[\"entity_matrix\"]  \n",
    "\n",
    "                    self.data.append(entry)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Get the words and single-word entity vector for the given index.\n",
    "        \n",
    "        Returns:\n",
    "            - words: List of words in the sentence.\n",
    "            - single_word_labels: Tensor with 1s for single-word entities, 0s otherwise.\n",
    "        \"\"\"\n",
    "        entry = self.data[idx]\n",
    "        words = entry[\"words\"]  # List of words\n",
    "        entities = entry[\"entities\"]\n",
    "        entity_matrix = torch.tensor(entry[\"entity_matrix\"], dtype=torch.float32)\n",
    "        return words, entity_matrix, entities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel\n",
    "\n",
    "class EntityMatrixPredictor(nn.Module):\n",
    "    def __init__(self, bert_model_name=\"bert-base-cased\", hidden_dim=768, num_heads=4, dropout=0.1):\n",
    "        super(EntityMatrixPredictor, self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
    "\n",
    "        # MLP to process word embeddings before span classification\n",
    "        self.mlp_forward = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim), \n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Learnable vector for dot product projection\n",
    "        self.v_forward = nn.Parameter(torch.randn(hidden_dim))  # (hidden_dim,)\n",
    "\n",
    "        # MLP to process word embeddings before span classification\n",
    "        self.mlp_backward = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim), \n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Learnable vector for dot product projection\n",
    "        self.v_backward = nn.Parameter(torch.randn(hidden_dim))  # (hidden_dim,)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, word_ids):\n",
    "        batch_size, _ = input_ids.shape\n",
    "\n",
    "        # Step 1: Get BERT Token Embeddings\n",
    "        bert_output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        token_embeddings = bert_output.last_hidden_state  # (batch, seq_len, hidden_dim)\n",
    "\n",
    "        # Step 2: Aggregate Token Embeddings into Word Embeddings\n",
    "        max_words = max([max([wid for wid in word_id if wid is not None], default=-1) + 1 for word_id in word_ids])\n",
    "        word_embeddings = torch.zeros((batch_size, max_words, token_embeddings.shape[-1]), device=token_embeddings.device)\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            word_counts = torch.zeros((max_words, 1), device=token_embeddings.device)\n",
    "            for token_idx, word_idx in enumerate(word_ids[i]):\n",
    "                if word_idx is not None:\n",
    "                    word_embeddings[i, word_idx] += token_embeddings[i, token_idx]\n",
    "                    word_counts[word_idx] += 1\n",
    "\n",
    "            word_embeddings[i] /= word_counts.clamp(min=1)  # Avoid division by zero\n",
    "\n",
    "        # Step 4: Construct Word Pair Matrix (Concatenation of i-th and j-th word)\n",
    "        i_emb = word_embeddings.unsqueeze(2).expand(-1, -1, max_words, -1)  # (batch, max_words, max_words, hidden_dim)\n",
    "        j_emb = word_embeddings.unsqueeze(1).expand(-1, max_words, -1, -1)  # (batch, max_words, max_words, hidden_dim)\n",
    "        pair_matrix = torch.cat((i_emb, j_emb), dim=-1)  # (batch, max_words, max_words, hidden_dim * 2)\n",
    "\n",
    "        logits_forward = torch.matmul(self.mlp_forward(pair_matrix), self.v_forward)  # (batch, max_words, max_words)\n",
    "        logits_forward = torch.triu(logits_forward)\n",
    "\n",
    "        logits_backward = torch.matmul(self.mlp_backward(pair_matrix), self.v_backward)  # (batch, max_words, max_words)\n",
    "        logits_backward = torch.tril(logits_backward, diagonal=-1)\n",
    "\n",
    "        logits = logits_forward + logits_backward\n",
    "\n",
    "        return logits  # Raw logits (can be passed to BCEWithLogitsLoss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_archietcture = \"matrix_cadec\"\n",
    "\n",
    "dataset_dir = f\"./datasets/cadec\"\n",
    "model_path = f\"./models/{model_archietcture}_rl.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from transformers import BertTokenizer, AdamW\n",
    "import json\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "def get_train_loader(dataset_dir=dataset_dir):\n",
    "    # Ensure you are using a Fast Tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\", use_fast=True)\n",
    "\n",
    "    # Load dataset\n",
    "    train_dir = f\"{dataset_dir}/train\"  # Path to your dataset\n",
    "    train_dataset = EntityMatrixDataset(train_dir, tokenizer)\n",
    "\n",
    "    # Use the subset in the DataLoader\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=32,\n",
    "        shuffle=True,\n",
    "        collate_fn=lambda batch: collate_fn(batch, tokenizer)\n",
    "    )\n",
    "\n",
    "    return train_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def training_loop(model = EntityMatrixPredictor(), device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"), epochs = 3, pos_weight= 20, model_path=model_path, verbose = False, lambda_penalty = 0.1):    \n",
    "\n",
    "    model.to(device)\n",
    "    loss_bce = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([pos_weight], device=device))\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        train_loader = get_train_loader()\n",
    "\n",
    "        for batch in train_loader:\n",
    "            tokens = batch[0]\n",
    "            target_matrix = batch[1].to(device)  # Ensure targets are on the correct device\n",
    "\n",
    "            input_ids = tokens[\"input_ids\"].to(device)\n",
    "            attention_mask = tokens[\"attention_mask\"].to(device)\n",
    "\n",
    "            # Extract `word_ids` only once for the batch\n",
    "            word_ids = [tokens.word_ids(batch_index=i) for i in range(len(input_ids))]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass (now includes word_ids)\n",
    "            predicted_matrix = model(input_ids=input_ids, attention_mask=attention_mask, word_ids=word_ids)\n",
    "\n",
    "            # **Mask the target matrix to match valid words in predicted matrix**\n",
    "            batch_size, max_words, _ = predicted_matrix.shape\n",
    "            target_matrix = target_matrix[:, :max_words, :max_words]  # Trim to match predicted size\n",
    "\n",
    "            rewards = []\n",
    "            log_probs = []\n",
    "            probs = torch.sigmoid(predicted_matrix)\n",
    "\n",
    "            for i in range(probs.size(0)):  # Loop over batch\n",
    "                sampled_matrix = torch.bernoulli(probs[i]).detach().cpu()\n",
    "                reward = compute_loop_reward(sampled_matrix)\n",
    "                if verbose:\n",
    "                    print(f\"Reward: {reward}\")\n",
    "\n",
    "                rewards.append(reward)\n",
    "\n",
    "                log_p = (sampled_matrix * torch.log(probs[i] + 1e-6) +\n",
    "                        (1 - sampled_matrix) * torch.log(1 - probs[i] + 1e-6)).mean()\n",
    "                log_probs.append(log_p)\n",
    "\n",
    "            rewards = torch.tensor(rewards, device=probs.device)\n",
    "            rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-6)\n",
    "            rl_penalty = -torch.stack(log_probs) @ rewards\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"RL Penalty: {rl_penalty}\")\n",
    "\n",
    "            # Compute loss directly over the valid portion\n",
    "            loss = loss_bce(predicted_matrix, target_matrix) + lambda_penalty * rl_penalty\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        # Compute average loss for epoch\n",
    "        epoch_loss = total_loss / len(train_loader)\n",
    "        if verbose: print(f\"Epoch {epoch+1}, Loss: {epoch_loss}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward: 0.0\n",
      "Reward: 0.0\n",
      "Reward: 0.0\n",
      "Reward: 0.0\n",
      "Reward: 0.0\n",
      "Reward: 0.0\n",
      "Reward: 0.0\n",
      "Reward: 0.0\n",
      "Reward: 0.0\n",
      "Reward: 0.0\n",
      "Reward: 0.0\n",
      "Reward: 0.0\n",
      "Reward: 0.0\n",
      "Reward: 0.0\n",
      "Reward: 0.0\n",
      "Reward: 0.0\n",
      "Reward: 0.0\n",
      "Reward: 0.0\n",
      "Reward: 0.0\n",
      "Reward: 0.0\n",
      "Reward: 0.0\n",
      "Reward: 0.0\n",
      "Reward: 0.0\n",
      "Reward: 0.0\n",
      "Reward: 0.0\n",
      "Reward: 0.0\n",
      "Reward: 0.0\n",
      "Reward: 0.0\n",
      "Reward: 0.0\n",
      "Reward: 0.0\n",
      "Reward: 0.0\n",
      "Reward: 0.0\n",
      "RL Penalty: 0.0\n",
      "Reward: 0.0\n",
      "Reward: 0.0\n",
      "Reward: 0.0\n",
      "Reward: 0.0\n",
      "Reward: 0.0\n",
      "Reward: 0.0\n",
      "Reward: 0.0\n",
      "Reward: 0.0\n",
      "Reward: 0.017241379246115685\n",
      "Reward: 0.008620689623057842\n",
      "Reward: 0.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[424], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtraining_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mstate_dict(), model_path)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel saved at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[423], line 41\u001b[0m, in \u001b[0;36mtraining_loop\u001b[0;34m(model, device, epochs, pos_weight, model_path, verbose, lambda_penalty)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(probs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)):  \u001b[38;5;66;03m# Loop over batch\u001b[39;00m\n\u001b[1;32m     40\u001b[0m     sampled_matrix \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbernoulli(probs[i])\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[0;32m---> 41\u001b[0m     reward \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_loop_reward\u001b[49m\u001b[43m(\u001b[49m\u001b[43msampled_matrix\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m verbose:\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReward: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreward\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[410], line 34\u001b[0m, in \u001b[0;36mcompute_loop_reward\u001b[0;34m(pred_matrix)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m split_matrices:\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m0.0\u001b[39m, device\u001b[38;5;241m=\u001b[39mpred_matrix\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m---> 34\u001b[0m loop_flags \u001b[38;5;241m=\u001b[39m [find_loops_torch(m) \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m split_matrices]\n\u001b[1;32m     35\u001b[0m nodes_with_loops \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(loop_flags)\u001b[38;5;241m.\u001b[39many(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     36\u001b[0m nodes_with_edges \u001b[38;5;241m=\u001b[39m nodes_with_arcs_torch(pred_binary)\n",
      "Cell \u001b[0;32mIn[410], line 34\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m split_matrices:\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m0.0\u001b[39m, device\u001b[38;5;241m=\u001b[39mpred_matrix\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m---> 34\u001b[0m loop_flags \u001b[38;5;241m=\u001b[39m [\u001b[43mfind_loops_torch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m split_matrices]\n\u001b[1;32m     35\u001b[0m nodes_with_loops \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(loop_flags)\u001b[38;5;241m.\u001b[39many(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     36\u001b[0m nodes_with_edges \u001b[38;5;241m=\u001b[39m nodes_with_arcs_torch(pred_binary)\n",
      "Cell \u001b[0;32mIn[410], line 8\u001b[0m, in \u001b[0;36mfind_loops_torch\u001b[0;34m(adj_matrix)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind_loops_torch\u001b[39m(adj_matrix):\n\u001b[1;32m      7\u001b[0m     n \u001b[38;5;241m=\u001b[39m adj_matrix\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m----> 8\u001b[0m     loop_diag \u001b[38;5;241m=\u001b[39m [reachability_torch(adj_matrix, i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, n \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)]\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mstack(loop_diag)\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mbool()\n",
      "Cell \u001b[0;32mIn[410], line 8\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind_loops_torch\u001b[39m(adj_matrix):\n\u001b[1;32m      7\u001b[0m     n \u001b[38;5;241m=\u001b[39m adj_matrix\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m----> 8\u001b[0m     loop_diag \u001b[38;5;241m=\u001b[39m [\u001b[43mreachability_torch\u001b[49m\u001b[43m(\u001b[49m\u001b[43madj_matrix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, n \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)]\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mstack(loop_diag)\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mbool()\n",
      "Cell \u001b[0;32mIn[410], line 4\u001b[0m, in \u001b[0;36mreachability_torch\u001b[0;34m(m, k)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreachability_torch\u001b[39m(m, k):\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mdiag(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatrix_power\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = training_loop(verbose=True)\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(f\"model saved at {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def reachability_torch(m, k):\n",
    "    return torch.diag(torch.matrix_power(m, k))\n",
    "\n",
    "def find_loops_torch(adj_matrix):\n",
    "    n = adj_matrix.shape[0]\n",
    "    loop_diag = [reachability_torch(adj_matrix, i) for i in range(1, n + 1)]\n",
    "    return torch.stack(loop_diag).sum(dim=0).bool()  # shape: (n,)\n",
    "\n",
    "def split_lower_triangular_ones_torch(matrix):\n",
    "    upper_triangle = torch.triu(matrix)\n",
    "    lower_positions = (torch.tril(matrix, diagonal=-1) == 1).nonzero(as_tuple=False)\n",
    "\n",
    "    matrices = []\n",
    "    for pos in lower_positions:\n",
    "        i, j = pos\n",
    "        new_matrix = upper_triangle.clone()\n",
    "        new_matrix[i, j] = 1\n",
    "        matrices.append(new_matrix)\n",
    "\n",
    "    return matrices\n",
    "\n",
    "def nodes_with_arcs_torch(adj_matrix):\n",
    "    return (adj_matrix.sum(dim=0) > 0) | (adj_matrix.sum(dim=1) > 0)\n",
    "\n",
    "def compute_loop_reward(pred_matrix):\n",
    "    pred_binary = (pred_matrix > 0.5).int()\n",
    "\n",
    "    split_matrices = split_lower_triangular_ones_torch(pred_binary)\n",
    "    if not split_matrices:\n",
    "        return torch.tensor(0.0, device=pred_matrix.device)\n",
    "\n",
    "    loop_flags = [find_loops_torch(m) for m in split_matrices]\n",
    "    nodes_with_loops = torch.stack(loop_flags).any(dim=0)\n",
    "    nodes_with_edges = nodes_with_arcs_torch(pred_binary)\n",
    "\n",
    "    # XNOR equivalent: ~(A ^ B)\n",
    "    valid_nodes = ~(nodes_with_loops ^ (~nodes_with_edges))\n",
    "    num_valid = valid_nodes.sum().float()\n",
    "\n",
    "    return num_valid / pred_matrix.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_spans_from_matrix(matrix):\n",
    "    \"\"\"\n",
    "    Extracts and merges entity spans by collapsing linked words in the **upper triangular** part of the entity matrix.\n",
    "\n",
    "    Args:\n",
    "        matrix (torch.Tensor): Binary entity matrix (size: max_words x max_words).\n",
    "\n",
    "    Returns:\n",
    "        merged_spans (set of tuples): Extracted entity spans in (start, end) format.\n",
    "    \"\"\"\n",
    "    max_words = matrix.shape[0]\n",
    "    spans = []\n",
    "\n",
    "    # **Step 1: Extract Raw Spans from Upper Triangle**\n",
    "    for i in range(max_words):\n",
    "        if matrix[i, i] == 1:\n",
    "                spans.append([i, i])\n",
    "\n",
    "    start = -1\n",
    "    for i in range(max_words-1):\n",
    "        if matrix[i,i+1] == 1:\n",
    "            if start == -1:\n",
    "                start = i\n",
    "        elif start != -1:\n",
    "            spans.append([start,i])\n",
    "            start = -1\n",
    "    if start != -1:\n",
    "         spans.append([start,max_words-1])\n",
    "\n",
    "    return spans  # Convert to set for unique values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "def evaluation_loop(model_path):\n",
    "    \"\"\"\n",
    "    Evaluates the model by directly comparing the predicted and target matrices.\n",
    "\n",
    "    Args:\n",
    "        model_path (str): Path to the trained model.\n",
    "    \n",
    "    Returns:\n",
    "        Precision, Recall, F1 Score computed at the matrix level.\n",
    "    \"\"\"\n",
    "    # **Initialize tokenizer**\n",
    "    bert_model_name = \"bert-base-cased\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(bert_model_name, use_fast=True)\n",
    "\n",
    "    # **Create the evaluation dataset**\n",
    "    eval_dir = f\"{dataset_dir}/validation\"  # Replace with your validation directory path\n",
    "    eval_dataset = EntityMatrixDataset(eval_dir, tokenizer)\n",
    "\n",
    "    # **Create the evaluation DataLoader**\n",
    "    eval_loader = DataLoader(\n",
    "        eval_dataset,\n",
    "        batch_size=8,  \n",
    "        shuffle=False,  \n",
    "        collate_fn=lambda batch: collate_fn(batch, tokenizer),  \n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    # **Load the trained model**\n",
    "    model = EntityMatrixPredictor(bert_model_name=\"bert-base-cased\")\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # **Store results**\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    # **Evaluation loop**\n",
    "    with torch.no_grad():\n",
    "        for batch in eval_loader:\n",
    "            tokens, target_matrices, _ = batch  \n",
    "\n",
    "            input_ids = tokens[\"input_ids\"].to(device)\n",
    "            attention_mask = tokens[\"attention_mask\"].to(device)\n",
    "\n",
    "            # **Extract word_ids for mapping token outputs to words**\n",
    "            word_ids = [tokens.word_ids(batch_index=i) for i in range(len(input_ids))]\n",
    "\n",
    "            # **Model prediction**\n",
    "            predicted_matrix = model(input_ids=input_ids, attention_mask=attention_mask, word_ids=word_ids)\n",
    "\n",
    "            # **Convert logits to binary predictions**\n",
    "            binary_predictions = (predicted_matrix > 0.5).long()\n",
    "\n",
    "            # **Flatten matrices for metric computation**\n",
    "            for i in range(len(input_ids)):\n",
    "                max_word_idx = max([wid for wid in word_ids[i] if wid is not None], default=-1) + 1\n",
    "\n",
    "                # Extract the relevant part of the matrix\n",
    "                pred_matrix = binary_predictions[i, :max_word_idx, :max_word_idx]\n",
    "                target_matrix = target_matrices[i, :max_word_idx, :max_word_idx]\n",
    "\n",
    "                # Flatten and store\n",
    "                all_preds.extend(pred_matrix.cpu().numpy().flatten())\n",
    "                all_targets.extend(target_matrix.cpu().numpy().flatten())\n",
    "\n",
    "    # **Compute Metrics**\n",
    "    precision = precision_score(all_targets, all_preds)\n",
    "    recall = recall_score(all_targets, all_preds)\n",
    "    f1 = f1_score(all_targets, all_preds)\n",
    "\n",
    "    return precision, recall, f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/76/0q1zg88x6rx9cckhgjcbyh540000gn/T/ipykernel_14804/1573139335.py:35: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results:\n",
      "Precision: 0.0000, Recall: 0.0000, F1 Score: 0.0000\n"
     ]
    }
   ],
   "source": [
    "precision, recall, f1 = evaluation_loop(model_path=model_path)\n",
    "\n",
    "print(f\"Evaluation Results:\")\n",
    "print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
