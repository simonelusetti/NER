{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward: 1.5438597202301025\n",
      "Reward: 1.6666667461395264\n",
      "Reward: 1.2017543315887451\n",
      "Reward: 1.675438642501831\n",
      "Reward: 1.5789473056793213\n",
      "Reward: 1.8947367668151855\n",
      "Reward: 1.824561357498169\n",
      "Reward: 0.8508771657943726\n",
      "Reward: 1.4210525751113892\n",
      "Reward: 1.1403508186340332\n",
      "Reward: 1.5877193212509155\n",
      "Reward: 1.8157894611358643\n",
      "Reward: 1.6140351295471191\n",
      "Reward: 1.9912281036376953\n",
      "Reward: 1.6666667461395264\n",
      "Reward: 1.4649121761322021\n",
      "Reward: 0.9736841917037964\n",
      "Reward: 1.4824562072753906\n",
      "Reward: 1.5701754093170166\n",
      "Reward: 1.9210526943206787\n",
      "Reward: 1.736842155456543\n",
      "Reward: 1.8684210777282715\n",
      "Reward: 1.2456140518188477\n",
      "Reward: 1.175438642501831\n",
      "Reward: 1.763157844543457\n",
      "Reward: 0.9385964870452881\n",
      "Reward: 1.4649122953414917\n",
      "Reward: 1.7280702590942383\n",
      "Reward: 1.9649121761322021\n",
      "Reward: 0.8947368860244751\n",
      "Reward: 1.6666667461395264\n",
      "Reward: 1.4210526943206787\n",
      "Batch loss 0.5970603823661804\n",
      "Reward: 1.6810345649719238\n",
      "Reward: 0.9224137663841248\n",
      "Reward: 1.6379311084747314\n",
      "Reward: 1.965517282485962\n",
      "Reward: 0.8017240762710571\n",
      "Reward: 0.9482758045196533\n",
      "Reward: 1.6810345649719238\n",
      "Reward: 1.017241358757019\n",
      "Reward: 1.8362069129943848\n",
      "Reward: 1.034482717514038\n",
      "Reward: 1.1034482717514038\n",
      "Reward: 1.982758641242981\n",
      "Reward: 1.4741380214691162\n",
      "Reward: 1.3620688915252686\n",
      "Reward: 0.9224138259887695\n",
      "Reward: 0.9482758045196533\n",
      "Reward: 1.8879311084747314\n",
      "Reward: 1.5172414779663086\n",
      "Reward: 1.0086207389831543\n",
      "Reward: 1.594827651977539\n",
      "Reward: 1.0775861740112305\n",
      "Reward: 0.8448275923728943\n",
      "Reward: 1.7241380214691162\n",
      "Reward: 1.155172348022461\n",
      "Reward: 0.8534482717514038\n",
      "Reward: 0.9224138259887695\n",
      "Reward: 1.3879311084747314\n",
      "Reward: 0.9051724076271057\n",
      "Reward: 1.0775861740112305\n",
      "Reward: 0.931034505367279\n",
      "Reward: 1.5603448152542114\n",
      "Reward: 1.887930989265442\n",
      "Batch loss 0.6099528074264526\n",
      "Reward: 2.0\n",
      "Reward: 1.25\n",
      "Reward: 1.808333396911621\n",
      "Reward: 1.4583332538604736\n",
      "Reward: 1.6749999523162842\n",
      "Reward: 0.9583333730697632\n",
      "Reward: 1.433333396911621\n",
      "Reward: 1.7999999523162842\n",
      "Reward: 0.9833333492279053\n",
      "Reward: 1.1166666746139526\n",
      "Reward: 1.100000023841858\n",
      "Reward: 2.0\n",
      "Reward: 1.316666603088379\n",
      "Reward: 2.0\n",
      "Reward: 0.8333333134651184\n",
      "Reward: 1.4000000953674316\n",
      "Reward: 1.7916667461395264\n",
      "Reward: 2.0\n",
      "Reward: 1.0833332538604736\n",
      "Reward: 1.8666666746139526\n",
      "Reward: 1.375\n",
      "Reward: 1.558333396911621\n",
      "Reward: 1.1333333253860474\n",
      "Reward: 1.558333396911621\n",
      "Reward: 1.0499999523162842\n",
      "Reward: 1.5833332538604736\n",
      "Reward: 1.066666603088379\n",
      "Reward: 1.3666666746139526\n",
      "Reward: 0.8500000238418579\n",
      "Reward: 1.0416666269302368\n",
      "Reward: 1.1166666746139526\n",
      "Reward: 1.6166666746139526\n",
      "Batch loss 0.5491790175437927\n",
      "Reward: 1.669565200805664\n",
      "Reward: 1.95652174949646\n",
      "Reward: 1.0521738529205322\n",
      "Reward: 1.939130425453186\n",
      "Reward: 1.6173913478851318\n",
      "Reward: 1.3652174472808838\n",
      "Reward: 1.9652173519134521\n",
      "Reward: 1.895652174949646\n",
      "Reward: 0.8086956739425659\n",
      "Reward: 1.8434782028198242\n",
      "Reward: 0.991304337978363\n",
      "Reward: 1.3043477535247803\n",
      "Reward: 1.04347825050354\n",
      "Reward: 0.9130434393882751\n",
      "Reward: 1.91304349899292\n",
      "Reward: 1.600000023841858\n",
      "Reward: 0.947826087474823\n",
      "Reward: 1.226086974143982\n",
      "Reward: 1.8869564533233643\n",
      "Reward: 1.1739130020141602\n",
      "Reward: 1.773913025856018\n",
      "Reward: 1.2608695030212402\n",
      "Reward: 1.9652173519134521\n",
      "Reward: 1.7391303777694702\n",
      "Reward: 1.47826087474823\n",
      "Reward: 1.0695651769638062\n",
      "Reward: 1.834782600402832\n",
      "Reward: 1.2434782981872559\n",
      "Reward: 1.399999976158142\n",
      "Reward: 0.9304347634315491\n",
      "Reward: 1.078260898590088\n",
      "Reward: 1.5739130973815918\n",
      "Batch loss 0.6029893755912781\n",
      "Reward: 0.9661016464233398\n",
      "Reward: 1.3135592937469482\n",
      "Reward: 1.0254237651824951\n",
      "Reward: 1.0508475303649902\n",
      "Reward: 1.7711864709854126\n",
      "Reward: 1.1271185874938965\n",
      "Reward: 1.7881355285644531\n",
      "Reward: 1.3389830589294434\n",
      "Reward: 1.5762711763381958\n",
      "Reward: 1.7966101169586182\n",
      "Reward: 1.0\n",
      "Reward: 1.2627118825912476\n",
      "Reward: 1.6525423526763916\n",
      "Reward: 1.6610169410705566\n",
      "Reward: 1.1610169410705566\n",
      "Reward: 1.6525423526763916\n",
      "Reward: 1.3728814125061035\n",
      "Reward: 1.4152542352676392\n",
      "Reward: 0.9491525292396545\n",
      "Reward: 1.9661016464233398\n",
      "Reward: 1.7966101169586182\n",
      "Reward: 1.3050847053527832\n",
      "Reward: 1.2457627058029175\n",
      "Reward: 1.6610169410705566\n",
      "Reward: 1.101694941520691\n",
      "Reward: 1.6610169410705566\n",
      "Reward: 1.0254237651824951\n",
      "Reward: 1.0677965879440308\n",
      "Reward: 1.01694917678833\n",
      "Reward: 0.8559322357177734\n",
      "Reward: 1.4322034120559692\n",
      "Reward: 1.6779661178588867\n",
      "Batch loss 0.501397967338562\n",
      "Reward: 1.772357702255249\n",
      "Reward: 0.9756097793579102\n",
      "Reward: 1.1219512224197388\n",
      "Reward: 1.0487804412841797\n",
      "Reward: 1.0650405883789062\n",
      "Reward: 1.2357723712921143\n",
      "Reward: 1.5365853309631348\n",
      "Reward: 0.8861788511276245\n",
      "Reward: 1.8373984098434448\n",
      "Reward: 1.9756097793579102\n",
      "Reward: 1.317073106765747\n",
      "Reward: 1.658536672592163\n",
      "Reward: 1.3902438879013062\n",
      "Reward: 1.3577234745025635\n",
      "Reward: 1.2764227390289307\n",
      "Reward: 1.3333333730697632\n",
      "Reward: 1.9105690717697144\n",
      "Reward: 1.6910569667816162\n",
      "Reward: 1.3252032995224\n",
      "Reward: 1.5772358179092407\n",
      "Reward: 1.4634146690368652\n",
      "Reward: 0.7886179089546204\n",
      "Reward: 1.097561001777649\n",
      "Reward: 1.1463414430618286\n",
      "Reward: 1.642276406288147\n",
      "Reward: 1.642276406288147\n",
      "Reward: 1.1463414430618286\n",
      "Reward: 1.9674797058105469\n",
      "Reward: 0.9512194991111755\n",
      "Reward: 1.7967479228973389\n",
      "Reward: 1.8373982906341553\n",
      "Reward: 0.9674797058105469\n",
      "Batch loss 0.5063393712043762\n",
      "Reward: 0.8534482717514038\n",
      "Reward: 1.0862069129943848\n",
      "Reward: 1.7068965435028076\n",
      "Reward: 1.4741380214691162\n",
      "Reward: 1.594827651977539\n",
      "Reward: 1.1724138259887695\n",
      "Reward: 1.3879311084747314\n",
      "Reward: 2.0\n",
      "Reward: 1.6206896305084229\n",
      "Reward: 1.0\n",
      "Reward: 0.9568965435028076\n",
      "Reward: 1.732758641242981\n",
      "Reward: 0.91379314661026\n",
      "Reward: 2.0\n",
      "Reward: 1.1465517282485962\n",
      "Reward: 1.6120688915252686\n",
      "Reward: 1.8362069129943848\n",
      "Reward: 1.4482758045196533\n",
      "Reward: 1.155172348022461\n",
      "Reward: 1.4310345649719238\n",
      "Reward: 0.9913793206214905\n",
      "Reward: 1.732758641242981\n",
      "Reward: 1.405172348022461\n",
      "Reward: 1.715517282485962\n",
      "Reward: 1.7586207389831543\n",
      "Reward: 0.9655172228813171\n",
      "Reward: 0.9568965435028076\n",
      "Reward: 1.75\n",
      "Reward: 0.9913793206214905\n",
      "Reward: 1.655172348022461\n",
      "Reward: 0.9568965435028076\n",
      "Reward: 1.017241358757019\n",
      "Batch loss 0.5313941240310669\n",
      "Reward: 1.0254237651824951\n",
      "Reward: 0.8559322357177734\n",
      "Reward: 1.2457627058029175\n",
      "Reward: 0.8220338821411133\n",
      "Reward: 1.7457627058029175\n",
      "Reward: 0.9067796468734741\n",
      "Reward: 0.8220338821411133\n",
      "Reward: 0.7796609997749329\n",
      "Reward: 1.898305058479309\n",
      "Reward: 1.398305058479309\n",
      "Reward: 1.7203389406204224\n",
      "Reward: 1.5338983535766602\n",
      "Reward: 1.9322034120559692\n",
      "Reward: 0.9745762944221497\n",
      "Reward: 1.0677965879440308\n",
      "Reward: 1.5847457647323608\n",
      "Reward: 1.98305082321167\n",
      "Reward: 1.7288135290145874\n",
      "Reward: 1.6355931758880615\n",
      "Reward: 0.9576270580291748\n",
      "Reward: 1.01694917678833\n",
      "Reward: 1.991525411605835\n",
      "Reward: 1.0423729419708252\n",
      "Reward: 1.0\n",
      "Reward: 1.2372881174087524\n",
      "Reward: 0.813559353351593\n",
      "Reward: 1.7881355285644531\n",
      "Reward: 1.5932203531265259\n",
      "Reward: 1.98305082321167\n",
      "Reward: 1.9067796468734741\n",
      "Reward: 1.7796610593795776\n",
      "Reward: 1.1440677642822266\n",
      "Batch loss 0.5499851703643799\n",
      "Reward: 1.5882353782653809\n",
      "Reward: 1.1848739385604858\n",
      "Reward: 1.0672268867492676\n",
      "Reward: 1.0084034204483032\n",
      "Reward: 1.638655424118042\n",
      "Reward: 1.5378150939941406\n",
      "Reward: 1.151260495185852\n",
      "Reward: 1.4957983493804932\n",
      "Reward: 1.8823529481887817\n",
      "Reward: 1.6722688674926758\n",
      "Reward: 1.4033613204956055\n",
      "Reward: 0.9915966391563416\n",
      "Reward: 0.7647058963775635\n",
      "Reward: 0.9579831957817078\n",
      "Reward: 1.638655424118042\n",
      "Reward: 1.4453781843185425\n",
      "Reward: 1.8655462265014648\n",
      "Reward: 0.9915966987609863\n",
      "Reward: 1.697479009628296\n",
      "Reward: 1.7058823108673096\n",
      "Reward: 0.9495798349380493\n",
      "Reward: 0.7899159789085388\n",
      "Reward: 1.2857142686843872\n",
      "Reward: 1.4369747638702393\n",
      "Reward: 1.1092437505722046\n",
      "Reward: 1.0840336084365845\n",
      "Reward: 1.9327731132507324\n",
      "Reward: 1.8403360843658447\n",
      "Reward: 1.016806721687317\n",
      "Reward: 1.2857142686843872\n",
      "Reward: 1.7142857313156128\n",
      "Reward: 1.016806721687317\n",
      "Batch loss 0.4250293970108032\n",
      "Reward: 1.6470588445663452\n",
      "Reward: 1.5630252361297607\n",
      "Reward: 1.6722688674926758\n",
      "Reward: 1.4957983493804932\n",
      "Reward: 1.638655424118042\n",
      "Reward: 0.9159663915634155\n",
      "Reward: 0.8991596698760986\n",
      "Reward: 1.6050419807434082\n",
      "Reward: 1.0924370288848877\n",
      "Reward: 0.8235294222831726\n",
      "Reward: 1.6890757083892822\n",
      "Reward: 0.9243698120117188\n",
      "Reward: 1.1764706373214722\n",
      "Reward: 2.0\n",
      "Reward: 1.8739495277404785\n",
      "Reward: 0.9747899174690247\n",
      "Reward: 1.4621849060058594\n",
      "Reward: 1.1092437505722046\n",
      "Reward: 1.983193278312683\n",
      "Reward: 1.983193278312683\n",
      "Reward: 1.3949580192565918\n",
      "Reward: 1.6470588445663452\n",
      "Reward: 1.2352941036224365\n",
      "Reward: 0.7815126180648804\n",
      "Reward: 1.2521008253097534\n",
      "Reward: 1.0504201650619507\n",
      "Reward: 1.7142857313156128\n",
      "Reward: 0.9159663915634155\n",
      "Reward: 1.3865547180175781\n",
      "Reward: 0.9915966391563416\n",
      "Reward: 0.924369752407074\n",
      "Reward: 1.0\n",
      "Batch loss 0.473956823348999\n",
      "Reward: 1.1565217971801758\n",
      "Reward: 1.643478274345398\n",
      "Reward: 1.0347826480865479\n",
      "Reward: 1.2347825765609741\n",
      "Reward: 1.417391300201416\n",
      "Reward: 1.3043477535247803\n",
      "Reward: 0.8782609105110168\n",
      "Reward: 1.356521725654602\n",
      "Reward: 0.939130425453186\n",
      "Reward: 0.9739130735397339\n",
      "Reward: 0.852173924446106\n",
      "Reward: 1.669565200805664\n",
      "Reward: 1.6608695983886719\n",
      "Reward: 0.904347836971283\n",
      "Reward: 1.2347825765609741\n",
      "Reward: 1.6521739959716797\n",
      "Reward: 0.95652174949646\n",
      "Reward: 1.0695652961730957\n",
      "Reward: 0.7739130258560181\n",
      "Reward: 1.4695651531219482\n",
      "Reward: 1.3391304016113281\n",
      "Reward: 1.8695652484893799\n",
      "Reward: 1.921739101409912\n",
      "Reward: 1.226086974143982\n",
      "Reward: 1.56521737575531\n",
      "Reward: 1.669565200805664\n",
      "Reward: 2.0\n",
      "Reward: 1.0260869264602661\n",
      "Reward: 2.0\n",
      "Reward: 1.834782600402832\n",
      "Reward: 0.8434782028198242\n",
      "Reward: 1.1565217971801758\n",
      "Batch loss 0.4504159092903137\n",
      "Reward: 1.1666666269302368\n",
      "Reward: 1.1416666507720947\n",
      "Reward: 0.9249999523162842\n",
      "Reward: 1.4500000476837158\n",
      "Reward: 1.9666666984558105\n",
      "Reward: 1.683333396911621\n",
      "Reward: 1.4166667461395264\n",
      "Reward: 1.0833332538604736\n",
      "Reward: 1.9500000476837158\n",
      "Reward: 1.6583333015441895\n",
      "Reward: 1.0333333015441895\n",
      "Reward: 1.3166667222976685\n",
      "Reward: 1.0833333730697632\n",
      "Reward: 1.5\n",
      "Reward: 0.9750000238418579\n",
      "Reward: 1.399999976158142\n",
      "Reward: 1.9083333015441895\n",
      "Reward: 1.9500000476837158\n",
      "Reward: 1.7833333015441895\n",
      "Reward: 1.0499999523162842\n",
      "Reward: 1.5999999046325684\n",
      "Reward: 1.350000023841858\n",
      "Reward: 1.850000023841858\n",
      "Reward: 1.625\n",
      "Reward: 0.7250000238418579\n",
      "Reward: 0.9666666984558105\n",
      "Reward: 0.9750000238418579\n",
      "Reward: 1.316666603088379\n",
      "Reward: 0.8833333253860474\n",
      "Reward: 0.9333333373069763\n",
      "Reward: 0.9833333492279053\n",
      "Reward: 1.625\n",
      "Batch loss 0.4594913125038147\n",
      "Reward: 1.1119999885559082\n",
      "Reward: 1.0799999237060547\n",
      "Reward: 1.0799999237060547\n",
      "Reward: 1.0880000591278076\n",
      "Reward: 1.128000020980835\n",
      "Reward: 1.3840000629425049\n",
      "Reward: 0.9839999675750732\n",
      "Reward: 1.3680000305175781\n",
      "Reward: 1.7120000123977661\n",
      "Reward: 1.9119999408721924\n",
      "Reward: 1.559999942779541\n",
      "Reward: 1.343999981880188\n",
      "Reward: 1.1440000534057617\n",
      "Reward: 0.9120000004768372\n",
      "Reward: 0.9919999837875366\n",
      "Reward: 1.8559999465942383\n",
      "Reward: 2.0\n",
      "Reward: 0.9919999837875366\n",
      "Reward: 1.0080000162124634\n",
      "Reward: 1.472000002861023\n",
      "Reward: 0.9279999732971191\n",
      "Reward: 0.9679999947547913\n",
      "Reward: 1.3359999656677246\n",
      "Reward: 1.9040000438690186\n",
      "Reward: 1.0959999561309814\n",
      "Reward: 1.0080000162124634\n",
      "Reward: 1.8079999685287476\n",
      "Reward: 1.3279999494552612\n",
      "Reward: 0.9200000166893005\n",
      "Reward: 1.2239999771118164\n",
      "Reward: 1.7680000066757202\n",
      "Reward: 1.0399999618530273\n",
      "Batch loss 0.48940134048461914\n",
      "Reward: 1.232758641242981\n",
      "Reward: 0.9568965435028076\n",
      "Reward: 1.4482758045196533\n",
      "Reward: 1.112069010734558\n",
      "Reward: 1.4913792610168457\n",
      "Reward: 1.1379311084747314\n",
      "Reward: 1.3620688915252686\n",
      "Reward: 1.534482717514038\n",
      "Reward: 1.982758641242981\n",
      "Reward: 0.9396551847457886\n",
      "Reward: 0.9482758641242981\n",
      "Reward: 1.2931034564971924\n",
      "Reward: 1.1120688915252686\n",
      "Reward: 1.655172348022461\n",
      "Reward: 0.8448275923728943\n",
      "Reward: 1.7068965435028076\n",
      "Reward: 0.9051724076271057\n",
      "Reward: 1.9310344457626343\n",
      "Reward: 1.8189654350280762\n",
      "Reward: 1.0775861740112305\n",
      "Reward: 1.8706896305084229\n",
      "Reward: 1.844827651977539\n",
      "Reward: 1.4224138259887695\n",
      "Reward: 1.7068965435028076\n",
      "Reward: 0.9568965435028076\n",
      "Reward: 1.6293103694915771\n",
      "Reward: 1.844827651977539\n",
      "Reward: 1.8275861740112305\n",
      "Reward: 1.1982759237289429\n",
      "Reward: 0.9051724076271057\n",
      "Reward: 1.6810345649719238\n",
      "Reward: 0.9741379022598267\n",
      "Batch loss 0.533370316028595\n",
      "Reward: 0.939130425453186\n",
      "Reward: 1.6521738767623901\n",
      "Reward: 0.895652174949646\n",
      "Reward: 1.6608695983886719\n",
      "Reward: 1.0347826480865479\n",
      "Reward: 1.8695652484893799\n",
      "Reward: 1.165217399597168\n",
      "Reward: 1.5739130973815918\n",
      "Reward: 1.017391324043274\n",
      "Reward: 1.191304326057434\n",
      "Reward: 1.0956522226333618\n",
      "Reward: 2.0\n",
      "Reward: 1.3913042545318604\n",
      "Reward: 1.39130437374115\n",
      "Reward: 1.7826087474822998\n",
      "Reward: 1.6521739959716797\n",
      "Reward: 1.4869564771652222\n",
      "Reward: 1.626086950302124\n",
      "Reward: 0.991304337978363\n",
      "Reward: 1.539130449295044\n",
      "Reward: 1.921739101409912\n",
      "Reward: 1.982608675956726\n",
      "Reward: 1.539130449295044\n",
      "Reward: 1.460869550704956\n",
      "Reward: 1.5913043022155762\n",
      "Reward: 1.295652151107788\n",
      "Reward: 1.017391324043274\n",
      "Reward: 1.982608675956726\n",
      "Reward: 0.791304349899292\n",
      "Reward: 1.773913025856018\n",
      "Reward: 1.1565216779708862\n",
      "Reward: 1.7826087474822998\n",
      "Batch loss 0.5092781782150269\n",
      "Reward: 1.460869550704956\n",
      "Reward: 1.495652198791504\n",
      "Reward: 1.6608695983886719\n",
      "Reward: 1.252173900604248\n",
      "Reward: 1.017391324043274\n",
      "Reward: 1.808695673942566\n",
      "Reward: 1.626086950302124\n",
      "Reward: 1.878260850906372\n",
      "Reward: 1.5304348468780518\n",
      "Reward: 1.9652173519134521\n",
      "Reward: 1.9913043975830078\n",
      "Reward: 1.95652174949646\n",
      "Reward: 1.982608675956726\n",
      "Reward: 0.9652174115180969\n",
      "Reward: 1.330434799194336\n",
      "Reward: 1.2782608270645142\n",
      "Reward: 1.852173924446106\n",
      "Reward: 1.9478260278701782\n",
      "Reward: 1.2608695030212402\n",
      "Reward: 1.8260869979858398\n",
      "Reward: 1.295652151107788\n",
      "Reward: 1.6086957454681396\n",
      "Reward: 1.0347826480865479\n",
      "Reward: 1.121739149093628\n",
      "Reward: 0.8782609105110168\n",
      "Reward: 1.504347801208496\n",
      "Reward: 1.3478261232376099\n",
      "Reward: 1.208695650100708\n",
      "Reward: 1.626086950302124\n",
      "Reward: 1.2173912525177002\n",
      "Reward: 1.600000023841858\n",
      "Reward: 1.8434782028198242\n",
      "Batch loss 0.5408641695976257\n",
      "Reward: 0.9411764740943909\n",
      "Reward: 1.7647058963775635\n",
      "Reward: 1.7058823108673096\n",
      "Reward: 1.058823585510254\n",
      "Reward: 1.9075629711151123\n",
      "Reward: 1.0672268867492676\n",
      "Reward: 1.0924370288848877\n",
      "Reward: 1.848739504814148\n",
      "Reward: 0.8403361439704895\n",
      "Reward: 0.7815126180648804\n",
      "Reward: 0.8907563090324402\n",
      "Reward: 1.0084034204483032\n",
      "Reward: 1.1008403301239014\n",
      "Reward: 1.7394957542419434\n",
      "Reward: 1.210084080696106\n",
      "Reward: 1.2773109674453735\n",
      "Reward: 1.6470588445663452\n",
      "Reward: 1.4285714626312256\n",
      "Reward: 1.1764706373214722\n",
      "Reward: 1.2773109674453735\n",
      "Reward: 1.4873950481414795\n",
      "Reward: 2.0\n",
      "Reward: 0.9579831957817078\n",
      "Reward: 1.5798319578170776\n",
      "Reward: 1.7815126180648804\n",
      "Reward: 1.193277359008789\n",
      "Reward: 1.5462185144424438\n",
      "Reward: 0.7310924530029297\n",
      "Reward: 1.9579832553863525\n",
      "Reward: 1.0336134433746338\n",
      "Reward: 1.6302521228790283\n",
      "Reward: 1.0672268867492676\n",
      "Batch loss 0.45982059836387634\n",
      "Reward: 1.8050847053527832\n",
      "Reward: 0.7627118825912476\n",
      "Reward: 1.101694941520691\n",
      "Reward: 0.9830508828163147\n",
      "Reward: 1.0847457647323608\n",
      "Reward: 1.2033898830413818\n",
      "Reward: 1.4576270580291748\n",
      "Reward: 1.2033898830413818\n",
      "Reward: 1.6186439990997314\n",
      "Reward: 1.1949152946472168\n",
      "Reward: 1.3728814125061035\n",
      "Reward: 0.8644067645072937\n",
      "Reward: 1.771186351776123\n",
      "Reward: 0.8305084705352783\n",
      "Reward: 0.8559322357177734\n",
      "Reward: 1.8813560009002686\n",
      "Reward: 1.7118644714355469\n",
      "Reward: 1.9491524696350098\n",
      "Reward: 1.01694917678833\n",
      "Reward: 1.6271185874938965\n",
      "Reward: 1.398305058479309\n",
      "Reward: 0.9745762348175049\n",
      "Reward: 0.8813559412956238\n",
      "Reward: 1.1440677642822266\n",
      "Reward: 1.98305082321167\n",
      "Reward: 1.1440677642822266\n",
      "Reward: 1.9745762348175049\n",
      "Reward: 1.7457627058029175\n",
      "Reward: 1.0932203531265259\n",
      "Reward: 0.9406780004501343\n",
      "Reward: 1.8559322357177734\n",
      "Reward: 1.9745762348175049\n",
      "Batch loss 0.4591599404811859\n",
      "Reward: 1.9338842630386353\n",
      "Reward: 1.0661157369613647\n",
      "Reward: 1.0909090042114258\n",
      "Reward: 1.4876033067703247\n",
      "Reward: 1.719008207321167\n",
      "Reward: 1.0826447010040283\n",
      "Reward: 0.8429752588272095\n",
      "Reward: 1.0247933864593506\n",
      "Reward: 1.3388429880142212\n",
      "Reward: 1.2561984062194824\n",
      "Reward: 1.0165289640426636\n",
      "Reward: 1.0909090042114258\n",
      "Reward: 0.9752066135406494\n",
      "Reward: 1.2396694421768188\n",
      "Reward: 0.9256198406219482\n",
      "Reward: 0.8347107172012329\n",
      "Reward: 1.0909091234207153\n",
      "Reward: 1.4958677291870117\n",
      "Reward: 1.884297490119934\n",
      "Reward: 1.7603306770324707\n",
      "Reward: 1.9173552989959717\n",
      "Reward: 1.0413223505020142\n",
      "Reward: 1.0413223505020142\n",
      "Reward: 0.8264462947845459\n",
      "Reward: 1.454545497894287\n",
      "Reward: 0.8925619721412659\n",
      "Reward: 1.0165289640426636\n",
      "Reward: 1.8677685260772705\n",
      "Reward: 0.9338842630386353\n",
      "Reward: 0.9586777091026306\n",
      "Reward: 1.1900826692581177\n",
      "Reward: 1.0909090042114258\n",
      "Batch loss 0.3863646984100342\n",
      "Reward: 0.935064971446991\n",
      "Reward: 1.753246784210205\n",
      "Reward: 1.5584415197372437\n",
      "Reward: 1.0649350881576538\n",
      "Reward: 1.4675323963165283\n",
      "Reward: 0.9350649118423462\n",
      "Reward: 1.1948051452636719\n",
      "Batch loss 0.5111706852912903\n",
      "Epoch 1, Loss: 0.5073310792446136\n"
     ]
    }
   ],
   "source": [
    "from loop_loss_rl import training_loop_rl\n",
    "from NER_cadec import EntityMatrixPredictor\n",
    "import torch\n",
    "\n",
    "model = EntityMatrixPredictor(bert_model_name='bert-base-cased')\n",
    "model.load_state_dict(torch.load(\"./models/cadec/model_17.pth\"))\n",
    "model = training_loop_rl(\"./datasets/cadec\", model=model, verbose=True, pos_weight=17, epochs=1, rl_weight=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.save(model.state_dict(), \"./models/cadec/model_rlp_10.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.20902511707109409, 0.6680272108843538, 0.31841763942931256)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.23741529525653438, 0.6673469387755102, 0.3502320599785791)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from NER_cadec import evaluation_loop\n",
    "model = EntityMatrixPredictor(bert_model_name='bert-base-cased')\n",
    "model.load_state_dict(torch.load(\"./models/cadec/model_17.pth\"))\n",
    "print(evaluation_loop(\"./datasets/cadec\", model))\n",
    "model = EntityMatrixPredictor(bert_model_name='bert-base-cased')\n",
    "model.load_state_dict(torch.load(\"./models/cadec/model_rlp_10.pth\"))\n",
    "evaluation_loop(\"./datasets/cadec\", model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Config name is missing.\nPlease pick one among the available configs: ['Ade_corpus_v2_classification', 'Ade_corpus_v2_drug_ade_relation', 'Ade_corpus_v2_drug_dosage_relation']\nExample of usage:\n\t`load_dataset('ade_corpus_v2', 'Ade_corpus_v2_classification')`",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m dataset = \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43made_corpus_v2\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Or a specific community version if available\u001b[39;00m\n\u001b[32m      4\u001b[39m dataset[\u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m][\u001b[32m1\u001b[39m:\u001b[32m100\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/phd/NER/.venv/lib/python3.13/site-packages/datasets/load.py:2062\u001b[39m, in \u001b[36mload_dataset\u001b[39m\u001b[34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001b[39m\n\u001b[32m   2057\u001b[39m verification_mode = VerificationMode(\n\u001b[32m   2058\u001b[39m     (verification_mode \u001b[38;5;129;01mor\u001b[39;00m VerificationMode.BASIC_CHECKS) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m save_infos \u001b[38;5;28;01melse\u001b[39;00m VerificationMode.ALL_CHECKS\n\u001b[32m   2059\u001b[39m )\n\u001b[32m   2061\u001b[39m \u001b[38;5;66;03m# Create a dataset builder\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2062\u001b[39m builder_instance = \u001b[43mload_dataset_builder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2063\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2064\u001b[39m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2065\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2066\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2067\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2068\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2069\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2070\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2071\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2072\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2073\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2074\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2075\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_require_default_config_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   2076\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2077\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2079\u001b[39m \u001b[38;5;66;03m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[32m   2080\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m streaming:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/phd/NER/.venv/lib/python3.13/site-packages/datasets/load.py:1819\u001b[39m, in \u001b[36mload_dataset_builder\u001b[39m\u001b[34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, storage_options, trust_remote_code, _require_default_config_name, **config_kwargs)\u001b[39m\n\u001b[32m   1817\u001b[39m builder_cls = get_dataset_builder_class(dataset_module, dataset_name=dataset_name)\n\u001b[32m   1818\u001b[39m \u001b[38;5;66;03m# Instantiate the dataset builder\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1819\u001b[39m builder_instance: DatasetBuilder = \u001b[43mbuilder_cls\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1820\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1821\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1822\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1823\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1824\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1825\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mhash\u001b[39;49m\u001b[43m=\u001b[49m\u001b[43mdataset_module\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1826\u001b[39m \u001b[43m    \u001b[49m\u001b[43minfo\u001b[49m\u001b[43m=\u001b[49m\u001b[43minfo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1827\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1828\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1829\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1830\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mbuilder_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1831\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1832\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1833\u001b[39m builder_instance._use_legacy_cache_dir_if_possible(dataset_module)\n\u001b[32m   1835\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m builder_instance\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/phd/NER/.venv/lib/python3.13/site-packages/datasets/builder.py:343\u001b[39m, in \u001b[36mDatasetBuilder.__init__\u001b[39m\u001b[34m(self, cache_dir, dataset_name, config_name, hash, base_path, info, features, token, repo_id, data_files, data_dir, storage_options, writer_batch_size, **config_kwargs)\u001b[39m\n\u001b[32m    341\u001b[39m     config_kwargs[\u001b[33m\"\u001b[39m\u001b[33mdata_dir\u001b[39m\u001b[33m\"\u001b[39m] = data_dir\n\u001b[32m    342\u001b[39m \u001b[38;5;28mself\u001b[39m.config_kwargs = config_kwargs\n\u001b[32m--> \u001b[39m\u001b[32m343\u001b[39m \u001b[38;5;28mself\u001b[39m.config, \u001b[38;5;28mself\u001b[39m.config_id = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_builder_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    344\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    345\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcustom_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    346\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    347\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[38;5;66;03m# prepare info: DatasetInfo are a standardized dataclass across all datasets\u001b[39;00m\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# Prefill datasetinfo\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    352\u001b[39m     \u001b[38;5;66;03m# TODO FOR PACKAGED MODULES IT IMPORTS DATA FROM src/packaged_modules which doesn't make sense\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/phd/NER/.venv/lib/python3.13/site-packages/datasets/builder.py:555\u001b[39m, in \u001b[36mDatasetBuilder._create_builder_config\u001b[39m\u001b[34m(self, config_name, custom_features, **config_kwargs)\u001b[39m\n\u001b[32m    551\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m config_kwargs:\n\u001b[32m    552\u001b[39m         example_of_usage = (\n\u001b[32m    553\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mload_dataset(\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.repo_id\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01mor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mself\u001b[39m.dataset_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.BUILDER_CONFIGS[\u001b[32m0\u001b[39m].name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    554\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m555\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    556\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mConfig name is missing.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    557\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mPlease pick one among the available configs: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m.builder_configs.keys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    558\u001b[39m             + \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mExample of usage:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[33m`\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexample_of_usage\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    559\u001b[39m         )\n\u001b[32m    560\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    561\u001b[39m     builder_config = \u001b[38;5;28mself\u001b[39m.BUILDER_CONFIGS[\u001b[32m0\u001b[39m]\n",
      "\u001b[31mValueError\u001b[39m: Config name is missing.\nPlease pick one among the available configs: ['Ade_corpus_v2_classification', 'Ade_corpus_v2_drug_ade_relation', 'Ade_corpus_v2_drug_dosage_relation']\nExample of usage:\n\t`load_dataset('ade_corpus_v2', 'Ade_corpus_v2_classification')`"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"ade_corpus_v2\")  # Or a specific community version if available\n",
    "dataset[\"train\"][1:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_opening_paths(matrix, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Recursively find all maximal opening paths (as sequences of node indices) in the upper triangle of the matrix.\n",
    "\n",
    "    Args:\n",
    "        matrix (torch.Tensor): A square matrix of shape (N, N)\n",
    "        threshold (float): Minimum value to consider an arc active\n",
    "\n",
    "    Returns:\n",
    "        List[List[int]]: Only maximal (non-nested) paths represented as lists of node indices.\n",
    "    \"\"\"\n",
    "    n = matrix.size(0)\n",
    "    all_paths = []\n",
    "\n",
    "    def recurse(path):\n",
    "        i,j = path[-1]\n",
    "        branches = []\n",
    "        for k in range(i + 1, n):\n",
    "            if matrix[j, k] > threshold: \n",
    "                for m in range(k,n):\n",
    "                    if matrix[j, m] > threshold: \n",
    "                        branches.append((j, m))\n",
    "                        matrix[j, m] = 0\n",
    "                break\n",
    "        if branches == []: \n",
    "            all_paths.append(path)\n",
    "            return\n",
    "        for j,m in branches:\n",
    "            recurse(path + [(j,m)])\n",
    "\n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            if matrix[i, j] > threshold:\n",
    "                recurse([(i, j)])\n",
    "\n",
    "    return all_paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ordered_partitions(lst):\n",
    "    if not lst:\n",
    "        yield []\n",
    "        return\n",
    "    for i in range(1, len(lst) + 1):\n",
    "        first = lst[:i]\n",
    "        for rest in ordered_partitions(lst[i:]):\n",
    "            yield [first] + rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "def opening_paths_partitions(path):\n",
    "    \"\"\"\n",
    "    Given a forward path as list of arcs (i, j), generate all valid backward-closing partitions.\n",
    "    Each partition is a list of arcs like (to, from) that would close the entity.\n",
    "\n",
    "    Args:\n",
    "        path: List of tuples representing the forward arcs (e.g., [(0,1), (1,2), (2,3)])\n",
    "\n",
    "    Returns:\n",
    "        List[List[Tuple[int, int]]]: All possible backward arcs to close the path\n",
    "    \"\"\"\n",
    "    repairs = []\n",
    "    for p in ordered_partitions(path):\n",
    "        rep = []\n",
    "        for e in p:\n",
    "            rep.append((e[-1][1], e[0][0]))\n",
    "        repairs.append(rep)\n",
    "\n",
    "    return repairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repair_matrix_lower(logits, threshold=0.5):\n",
    "    upper = torch.triu(logits, diagonal=1)\n",
    "    paths = find_opening_paths(logits, threshold)\n",
    "    print(paths)\n",
    "    for path in paths:\n",
    "        partitions = opening_paths_partitions(path)\n",
    "        partitions_weight = []\n",
    "        for p in partitions:\n",
    "            partitions_weight.append(sum(list(map(lambda cell: 1-logits[cell[0],cell[1]], p))))\n",
    "\n",
    "        for repaired_cell in partitions[partitions_weight.index(min(partitions_weight))]:\n",
    "            logits[repaired_cell[0], repaired_cell[1]] = 1\n",
    "    return torch.tril(logits, diagonal=-1) + upper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(1, 3), (3, 4)]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.9000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.8000],\n",
       "        [0.0000, 1.0000, 0.0000, 0.0000, 0.0000]])"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = torch.tensor([\n",
    "    [0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "    [0.0, 0.0, 0.0, 0.9, 0.0],\n",
    "    [0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "    [0.0, 0.0, 0.0, 0.0, 0.8],\n",
    "    [0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "])\n",
    "\n",
    "repair_matrix_lower(logits, threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closing_paths(logits, treshold=0.5):\n",
    "    \"\"\"\n",
    "    Recursively find all maximal closing paths (as sequences of node indices) in the lower triangle of the matrix.\n",
    "\n",
    "    Args:\n",
    "        matrix (torch.Tensor): A square matrix of shape (N, N)\n",
    "        threshold (float): Minimum value to consider an arc active\n",
    "\n",
    "    Returns:\n",
    "        List[List[int]]: Only maximal (non-nested) paths represented as lists of node indices.\n",
    "    \"\"\"\n",
    "    n = logits.size(0)\n",
    "    all_paths = []\n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            if logits[j, i] > treshold:\n",
    "                all_paths.append((j,i))\n",
    "\n",
    "    return all_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closing_path_partitions(path):\n",
    "    i,j = path\n",
    "    partitions = []\n",
    "    for p in ordered_partitions(list(range(i - j))):\n",
    "        k,l = j, j\n",
    "        part = []\n",
    "        for e in p:\n",
    "            k = k + len(e)\n",
    "            part.append((l,k))\n",
    "            l = l + len(e)\n",
    "        partitions.append(part)\n",
    "    return partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repair_matrix_upper(logits, threshold=0.5):\n",
    "    lower = torch.tril(logits, diagonal=-1)\n",
    "    paths = find_closing_paths(logits, threshold)\n",
    "    for path in paths:\n",
    "        partitions = closing_path_partitions(path)\n",
    "        partitions_weight = []\n",
    "        for p in partitions:\n",
    "            partitions_weight.append(sum(list(map(lambda cell: 1-logits[cell[0],cell[1]], p))))\n",
    "        for repaired_cell in partitions[partitions_weight.index(min(partitions_weight))]:\n",
    "            logits[repaired_cell[0], repaired_cell[1]] = 1\n",
    "    return lower + torch.triu(logits, diagonal=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.8000, 0.3000, 1.0000],\n",
       "        [0.0000, 0.0000, 0.9000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.8000],\n",
       "        [0.9000, 0.0000, 0.0000, 0.0000]])"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = torch.tensor([\n",
    "    [0.0, 0.8, 0.3, 0.9],\n",
    "    [0.0, 0.0, 0.9, 0.0],\n",
    "    [0.0, 0.0, 0.0, 0.8],\n",
    "    [0.9, 0.0, 0.0, 0.0],\n",
    "])\n",
    "\n",
    "repair_matrix_upper(logits, threshold=0.5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
